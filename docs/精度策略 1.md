# 论文精读策略设计：带全局锚点的分块并行解析 (Anchored Map-Reduce)

## 1. 设计背景
针对学术论文（特别是包含复杂公式与约束逻辑的运筹学论文），传统的一次性输入（Long Context）容易导致模型在解析中段细节时出现“注意力涣散”。本策略通过“物理分块”保障细节精度，通过“全局锚点”消除语义断层，实现真正深度的算法拆解。



## 2. 核心架构定义
该架构分为四个阶段：**结构化解析 (Segmentation)**、**锚点固化 (Anchoring)**、**局部精读 (Map)**、**全局合成 (Reduce)**。

### 阶段一：结构化解析 (Structural Segmentation)
利用 Markdown 的标题层级（# , ##）将论文物理切分为以下预定义模块桶（Buckets）：
- **G-Anchor (核心锚点)**: Abstract + Introduction。
- **B1-Background**: Related Work + Preliminary。
- **B2-Methodology (硬核块)**: Model Formulation + Algorithm Design (包含决策变量、目标函数、约束条件、伪代码)。
- **B3-Experiments**: Results + Data Analysis (包含算力环境、指标对比)。
- **B4-Conclusion**: Future Work + Limitations。

### 阶段二：锚点固化 (Global Anchoring)
在并行处理开始前，首先提取并持久化 **Global Anchors**。
- **内容**: 标题、摘要、核心贡献点。
- **作用**: 作为每个分块解析任务的“背景常驻内存”，确保 AI 在读局部章节时始终对齐研究目标。

### 阶段三：局部精读 (Anchored Map)
对除锚点外的每个块进行独立提问，采用“背景+局部”的 Prompt 注入模式。
- **输入**: `[Global Anchors]` + `[Current Block Content]`。
- **指令示例**: "已知研究目标为 {Anchors}，请解构当前 {B2-Methodology} 章节。请精准提取所有约束逻辑及其物理意义。"
- **输出**: 高密度的局部见解片段 (Local Insights)。

### 阶段四：全局合成 (Final Reduce)
将所有局部见解按逻辑顺序聚合，进行终极逻辑自洽性校验。
- **核心逻辑**: 检查 B2 的方法论逻辑是否在 B3 的实验中得到闭环验证。
- **最终产出**: 结构化的精读报告、潜在改进点、算力开销总结。

## 3. 语义分块提取标准

| 模块类别 | 提取重点 (Extraction Focus) | 解决的痛点 |
| :--- | :--- | :--- |
| **Global Anchor** | 核心问题、拟解决的矛盾 | 消除分块后的上下文丢失 |
| **Methodology** | 决策变量、约束聚合逻辑、数学模型 | 解决 AI 容易忽略数学细节的问题 |
| **Algorithms** | 更新规则、计算复杂度、收敛性 | 解决伪代码理解浮于表面的问题 |
| **Experiments** | 算力平台 (如 5090)、Gap 指标、消减实验 | 验证算法性能的真实性与稳健型 |

## 4. 技术优势
1. **注意力强制聚焦**: 避免了长文本模型在处理大量实验数据时忘记前面的模型假设。
2. **低算力成本**: 每次推理仅需处理单个 Block 的 Token，支持本地小参数量模型（如 8B/14B）实现超长论文的精准解析。
3. **逻辑一致性**: 最后阶段的 Reduce 能够发现论文中“局部”与“全局”的不一致（即逻辑断层）。

---
*Generated by Gemini for paperRead Project Implementation.*